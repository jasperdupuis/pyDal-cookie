"""

Starting from script 1010, add frequency dependence to model

This file is only for 1d model training with freq dependency 
i.e., use only the y coordinate and frequency as features and 0-mean RLs as labels

major changes needed:
    add frequency to features and associated handling  ~DONE
    model complexity
    preprocess access all freq data, not single freq as in 1010

"""

import sys
import numpy as np
from matplotlib import pyplot as plt
import torch
import torch.nn as nn
from torch import optim
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset

import multiprocessing as mp

# For access to the real 0-mean spectral-xy dataset.
import pydal.models.SLR_with_transforms
import pydal._variables as _vars

# print("USING pytorch VERSION: ", torch.__version__)

# Control randomization
SEED        = 123456

# Define the data split
TRAIN       = 0.9
TEST        = 0.1
HOLDBACK    = 0.0


# Define the hyperparameters
EPOCHS              = 10
BATCH_SIZE          = 2**8
N_HIDDEN_NODES      = 2**9
N_HIDDEN_LAYERS     = 1 # not yet implemented
LEARNING_RATE       = 0.01
N_WORKERS           = 1
LEN_SMOOTHING       = 7


class DeepNetwork(nn.Module):
    '''
    A simple, general purpose, fully connected network

    The network topology is hardwired here, see __init__
    '''
    def __init__(self,):
        # Perform initialization of the pytorch superclass
        super(DeepNetwork, self).__init__()
        self.flatten = nn.Flatten()

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"\n\nModel using {self.device} device, tensors to be moved in loops if needed")
        
        neural_net = nn.Sequential(
              nn.Linear(2, N_HIDDEN_NODES),
               nn.ReLU(),
              nn.Linear(N_HIDDEN_NODES, N_HIDDEN_NODES),
              nn.ReLU(),
               nn.Linear(N_HIDDEN_NODES, N_HIDDEN_NODES),
                nn.ReLU(),
              nn.Linear(N_HIDDEN_NODES, N_HIDDEN_NODES),
                nn.ReLU(),
              nn.Linear(N_HIDDEN_NODES, N_HIDDEN_NODES),
                  nn.ReLU(),
              nn.Linear(N_HIDDEN_NODES, 1),
              )
        self.neural_net = neural_net.to(self.device)


    def forward(self, x):
        '''
        This method defines the network layering and activation functions
        '''
        x = self.neural_net(x) # see nn.Sequential
                
        return x
    

class f_yf_orca_dataset(torch.utils.data.Dataset):
    def __init__(self, x,outputs):
        """
        inputs and outputs are the features and labels as 1D tensors
        x for this file is a tuple with a 1d vector of y-values
        and a singleton freqeuncy value
        """
        y,f = x
        f = np.ones_like(y) * f
        x = y,f

        self.assign_basis_values (x,outputs)
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Model inputs and labels are being moved to {self.device} device.\n\n")


    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        """
        x and y are here inputs and labels, not cartesian
        
        this produces a batch based on indices generated by the parent class
        
        pre processing makes more sense here .... ?
        """
        inputs      = self.inputs[idx]
        labels      = self.labels[idx]
        
        # x           = torch.column_stack((y,f))
        
        inputs  = inputs.float()
        inputs  = inputs.to(self.device)
        labels  = labels.float()
        labels  = labels.to(self.device)
        return inputs, labels

    def assign_basis_values(self,x,outputs):
        """
        kwargs are feature identifiers in key-value pairs
        """
        # x = torch.tensor(x) # 
        # self.inputs = torch.column_stack((x))
        f           = torch.tensor(x[1]) 
        y           = torch.tensor(x[0])
        self.inputs = torch.column_stack((y,f))
        self.labels = torch.tensor(outputs)


def convolve1d_unstructured_x_y_data(x,y,kernel_size=LEN_SMOOTHING):
    """
    Assume x and y are unordered.
    For 1d, would want to arange values by x for x and y.
    
    x is the FEATURE
    y is the LABEL
    """
    x           = np.reshape(x,len(x))
    y           = np.reshape(y,len(y))
    sort_ind    = np.argsort(x)
    # x           = torch.Tensor(x[sort_ind])
    # y           = torch.Tensor(y[sort_ind])
    x           = x[sort_ind]
    y           = y[sort_ind]
    kern        = torch.ones(kernel_size) / kernel_size
    x           = torch.Tensor ( np.convolve(x,kern,mode='valid') )
    y           = torch.Tensor ( np.convolve(y,kern,mode='valid') )
    x,y         = x.unsqueeze(1),y.unsqueeze(1) # set to dimensions [n,1]
    return x,y    

    
def train( loader_train, loader_val, batch_size = 100, epochs=5):
    """
    # Same story as train_batch...
    
    loader = train_dataloader
    batch_Size = 100
    epochs = 5
    e = 1
    """        
    # losses = np.zeros(int(epochs*batch_size))
    loss_e_train = []
    loss_e_val = []
    batch_index = 0
    for e in range(epochs):
        losses_t = []
        losses_v = []
        for i,data in enumerate(loader_train):
            features,labels = data
            y,f             = features[:,0],features[:,1]
            f               = torch.ones_like(y) * f[0]
            features        = torch.column_stack((y,f))

            # Zero the gradients
            optimizer.zero_grad()

            # # Run forward calculation        
            y_predict = model(features)
            
            # Compute loss.
            loss_t = loss_fn(y_predict, labels)
            
            # Backward pass: compute gradient of the loss with respect to model
            # parameters
            loss_t.backward()
            
            # Calling the step function on an Optimizer makes an update to its
            # parameters
            optimizer.step()
            del features
            del labels
            batch_index += 1
            losses_t.append(loss_t)

        for i, data in enumerate(loader_val):
            features,labels = data
            
            # Forward Pass
            outputs = model(features)
            # Find the Loss
            validation_loss = loss_fn(outputs, labels)
            # Calculate Loss
            losses_v.append(validation_loss.item())
        
        loss_e_val.append(losses_v)
        loss_e_train.append(losses_t)
        print("Epoch: ", e+1)
        print("Batches: ", batch_index)

    return loss_e_train,loss_e_val


if __name__ == "__main__":        
    # mp.freeze_support()

    # Repeatability:
    fixed_seed  = torch.manual_seed(SEED)

    fname2019   = r'concatenated_data_2019.pkl'
    fname2020   = r'concatenated_data_2020.pkl'
    data2019    = pydal.models.SLR_with_transforms.load_concat_arrays(fname2019)
    data2020    = pydal.models.SLR_with_transforms.load_concat_arrays(fname2020)
    data        = pydal.utils.concat_dictionaries(data2019,data2020)

    f           = data['Frequency']
    rl_s        = data['South'] # 2d array, zero mean gram
    rl_n        = data['North'] # 2d array, zero mean gram
    rl_s        = rl_s / _vars.RL_SCALING #normalize to roughly -1/1    
    rl_n        = rl_n / _vars.RL_SCALING #normalize to roughly -1/1    
    x           = data['X'] / _vars.X_SCALING
    y           = data['Y'] / _vars.Y_SCALING


    # freq_targ   = 105
    freq_targ       = 55
    # freq_targ       = 80


    fname2019   = r'concatenated_data_2019.pkl'
    fname2020   = r'concatenated_data_2020.pkl'
    # data2019    = pydal.models.SLR_with_transforms.load_concat_arrays(fname2019)
    data2020    = pydal.models.SLR_with_transforms.load_concat_arrays(fname2019)
        
    # Assign features and labels here. 
    labels          = received_level # 1D numpy array
    
    
    # train data
    dset_full               = classes.f_y_orca_dataset(y, labels)
    test_size               = int ( len(y) * _vars.TEST)
    hold_size               = int ( len(y) * _vars.HOLDBACK)
    train_size              = len(dset_full) - test_size - hold_size
    dset_train,dset_test,dset_hold    = \
        torch.utils.data.random_split(
            dataset     = dset_full,
            lengths     = [train_size,test_size,hold_size],
            generator   = fixed_seed  )
    
    
    # Sampler sets the distribution for selection for a batch of data
    weights = np.ones(len(dset_train)) / len(dset_train)
    sampler = torch.utils.data.WeightedRandomSampler(
        weights, 
        len(dset_train), 
        replacement=True)
    
    # Create the dataloaders from the datasets
    dataloader_train = DataLoader(
            dset_train, 
            batch_size=BATCH_SIZE,
            # sampler = sampler,
            num_workers = N_WORKERS) 
    dataloader_test = DataLoader(
            dset_test, 
            batch_size=BATCH_SIZE,
            # sampler = sampler,
            num_workers = N_WORKERS) 
    
    
    # Instantiate the network (hardcoded in class)
    model = DeepNetwork()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    loss_fn = nn.MSELoss()  # mean squared error    
    
    
    losses_t, losses_v = train(
        dataloader_train,
        dataloader_test,
        BATCH_SIZE,
        EPOCHS
        )
    
    # visualize loss profiles per epoch (Takes last batch loss):
    losses_train_unravel    = np.zeros(len(losses_t)) # epoch length
    index = 0
    for l in losses_t:
        losses_train_unravel[index] = l[-1]           
        index += 1
    losses_val_unravel    = np.zeros(len(losses_v) ) # epoch length
    index = 0
    for l in losses_v:
        losses_val_unravel[index] = l[-1]            
        index += 1
    plt.figure();
    plt.plot(losses_train_unravel,label='train loss')
    plt.plot(losses_val_unravel,label='validation loss')
    plt.suptitle('End of epoch losses')
    plt.legend()
    plt.show()
    
   
    # Visualize results.
    #Set up the cartesian geometry
    xmin = -1
    xmax = 1
    ymin = -1
    ymax = 1
    
    x_range     = np.arange(xmin,xmax,step=0.01)
    y_range     = np.arange(ymin,ymax,step=0.01)
    x_size      = xmax - xmin
    y_size      = ymax - ymin
    x_surface   = np.ones((y_size,x_size)) # dim1 is column index, dim2 is row index
    y_surface   = (x_surface[:,:].T * np.arange(ymin,ymax)*-1).T # hackery to use the numpy functions, no big deal
    x_surface   = x_surface[:,:] * np.arange(xmin,xmax)
    
    model.eval()
    test        = torch.tensor(y_range)
    result      = []
    freq        = torch.tensor(f[f_index]).float()
    with torch.no_grad():
        for t in test:
            t = t.float()
            t = t.reshape((1,1))
            inputs = torch.column_stack((t,freq))
            result.append(model.neural_net(inputs))
        
    
    yy, rr      = convolve1d_unstructured_x_y_data(y,rl_s[f_index,:])
    plt.figure() ; plt.scatter(yy,rr,marker='.')
    plt.plot(y_range,result,color='red')
    # plt.figure() ; plt.scatter(y,np.convolve(received_level,np.ones(11)/11,mode='same'),marker='.')


